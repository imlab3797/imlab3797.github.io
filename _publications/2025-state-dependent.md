---
type: "Conference Paper"
layout: publication
group: publications
title: "State-Dependent Lagrange Multipliers for State-Wise Safety in Constrained Reinforcement Learning"
authors:
  - name: "Minseok Seo"
  - name: "Kyunghwan Choi"
    corresponding: true # true if this author is the corresponding author
domestic_or_international: "International" # or "domestic"
preprint: 
  - name: "TechRxiv"
    doi: "10.36227/techrxiv.175979326.64150311/v1"
    pdf: "/static/pub/2025-state-dependent.pdf"
    year: "2025"
    state: "published"
pub:
  - name: "International Conference on Robot Intelligence Technology and Applications (RiTA)"
    doi: 
    year: "2025"
    state: "accepted"
pub_date: "2025-12-31"
image: "/static/pub/2025-state-dependent.png"
abstract: "
 Despite the remarkable success of deep reinforcement learning (RL) across various domains, its deployment in the real world remains limited due to safety concerns. To address this challenge, constrained RL has been extensively studied as an approach for learning safe policies while maintaining performance. However, since constrained RL enforces constraints in the form of cumulative costs, it cannot guarantee state-wise safety. In this paper, we extend the Lagrangian based approach, a representative method in constrained RL, by introducing state-dependent Lagrange multipliers so that the policy is trained to account for state-wise safety. ã„´Our results show that the proposed method enables more fine-grained specification of the constraints and allows the policy to satisfy them more effectively by employing state-dependent Lagrange multipliers instead of a single scalar multiplier.
"
# links:
#   - name: 
#     url: 
---
